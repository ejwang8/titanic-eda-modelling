{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4f75e1",
   "metadata": {},
   "source": [
    "## How can we predict survival on the Titanic?\n",
    "\n",
    "to do list for self\n",
    "- 1 apply fcc principles\n",
    "- 2 apply [https://www.linkedin.com/pulse/what-i-learned-analyzing-famous-titanic-dateset-murilo-gustineli/]\n",
    "- 3 apply [https://python.plainenglish.io/revitalizing-cyclistic-bike-share-program-an-in-depth-data-exploration-556b52512bf8] - diff dataset but still\n",
    "- 4 apply others? [https://www.kaggle.com/code/startupsci/titanic-data-science-solutions]\n",
    "\n",
    "### Guiding Questions\n",
    "- Which features are correlated with survival?\n",
    "- Can we accurately predict survival with a simple model?\n",
    "- Which model performs best on this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f607ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train and test datasets and find length, info(), and describe() summary statistics\n",
    "train_df = pd.read_csv('titanic/train.csv')\n",
    "test_df = pd.read_csv('titanic/test.csv')\n",
    "print(f\"Passengers in train set: {train_df.shape[0]}\\nPassengers in test set: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Cabin\"].str[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f15ce5",
   "metadata": {},
   "source": [
    "891:418 which roughly equals a 7:3 ratio of train:test rows (before dropping any training rows as needed???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024c3ce",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "| Column | dtype | Description |\n",
    "|----------|----------|----------|\n",
    "| PassengerId  | int64  | unique passenger  |\n",
    "| Survived  | int64  | binary value of survival outcome (0, 1)  |\n",
    "| Pclass  | int64  | class (1, 2, 3)  |\n",
    "| Name  | object  | string value for name --> to quantify in feature engineering, we deduced title from this as well as name length  |\n",
    "| Sex  | object  | sex (\"male\", \"female\")  |\n",
    "| Age  | float64  | passenger's age at time of ?  |\n",
    "| SibSp  | int64  | sibling/spouse #?  |\n",
    "| Parch  | int64  | parents/children #?  |\n",
    "| Ticket  | object?  | ?  |\n",
    "| Fare  | float64  | ticket cost  |\n",
    "| Cabin  | object  | cabin identifer of format letter + number (e.g. C85, C123, B42, C148) - from research, letter corresponds to deck which we engineered our own feature from this  |\n",
    "| Embarked  | object  | port passenger boarded from? ('S', 'C', 'Q' which correspond to Southampton, Cherbourg, Queenstown)  |\n",
    "\n",
    "NOTE - variations to help algorithm?\n",
    "- deal with fare, cabin, age dropped values differently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2aaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860068b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeedb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe621a",
   "metadata": {},
   "source": [
    "We see that the Age, Cabin, and Embarked have null values to be dealt with/cleaned in the train datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b24bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c93929",
   "metadata": {},
   "source": [
    "We see that the Age, Fare, Cabin have null values to be dealt with/cleaned in the train datset\n",
    "\n",
    "Next, let's look at statistic summaries of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b649f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c756f52",
   "metadata": {},
   "source": [
    "We see train and test df have similar summary statistics. from first glance, all columns seem to have reasonable means and min/max (pclass 1-3 all valid, age in right range, # sibs or parch as well as fare all seem reasonable with no outright impossible valus/outliers - though fare seems to be very right skewed)\n",
    "\n",
    "we will look into each variable distribution more to see if any single varaible distribution can be preprocessed or cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f80ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "print(f\"Minimum age: {train_df['Age'].min()}. Maximum age: {train_df['Age'].max()}.\")\n",
    "print(f\"Duplicate PassengerIds? {train_df.duplicated('PassengerId').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac8c81",
   "metadata": {},
   "source": [
    "First let's figure out to deal with missing AGE and CABIN columns which there are a lot of in train and test. but we'll only look at train set??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39bd20",
   "metadata": {},
   "source": [
    "## Visualizing each variable (pre cleaning/n/a values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot for each categorical attributes cumulative\n",
    "#AND sns.countplot for each categorical attributes by died/survived\n",
    "    #survived, sex, embarked\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 3), nrows=1, ncols=3)\n",
    "sns.countplot(x=\"Survived\", data=train_df, ax=axes[0])\n",
    "sns.countplot(x=\"Sex\", data=train_df, ax=axes[1])\n",
    "sns.countplot(x=\"Embarked\", data=train_df, ax=axes[2])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46d8e3",
   "metadata": {},
   "source": [
    "We see that majority perished in titanic, there were more male aboard, and most embarked from 'S' in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50363eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot for numerical attirbutes\n",
    "#pclass, age, sibsp, parch, fare\n",
    "\n",
    "fig1, axes1 = plt.subplots(figsize=(8, 3), nrows=1, ncols=3)\n",
    "fig2, axes2 = plt.subplots(figsize=(8, 3), nrows=1, ncols=2)\n",
    "sns.countplot(x=\"Pclass\", data=train_df, ax=axes1[0])\n",
    "sns.histplot(x=\"Age\", data=train_df, ax=axes1[1])\n",
    "sns.countplot(x=\"SibSp\", data=train_df, ax=axes1[2])\n",
    "sns.countplot(x=\"Parch\", data=train_df, ax=axes2[0])\n",
    "sns.histplot(x=\"Fare\", data=train_df, ax=axes2[1])\n",
    "fig1.tight_layout()\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa437cd8",
   "metadata": {},
   "source": [
    "majority of passengers in 3rd class, then 1st, then 2nd. age seems normally distributed (???). sib sp and parch seem similar right skewed most had 0. fare seems very right skewed so may need to be normalized(???) - example used log but other methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96130ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c0a723",
   "metadata": {},
   "source": [
    "for managing cabin, since it's a string we'll just set to \"Unknown\" for fillna. it seems that one thing we can feature engineeri extract the first letter which is the \"deck\" that a passenger stayed on so\n",
    "- deck fillna with \"Unknown\"\n",
    "- we will create a \"deck\" which is the first letter of the \"cabin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Deck\"] = train_df[\"Cabin\"].str[0] # U = unknown\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2aa283",
   "metadata": {},
   "source": [
    "feature engineering for NAME\n",
    "- the \"name\" is of format \"[surname], [title], [first and middle etc name/nicknames]\" which we can deduce \"surname length\", \"title\" and \"first name length\" from which i imagine are more meaningful features than name - so i will deduce \"surname\" and \"title\" and then drop calculate teh lenght of the remaining first name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109607b5",
   "metadata": {},
   "source": [
    "TICKET\n",
    "- will also drop because seems unhelpful/unstandardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Surname Length\"] = train_df[\"Name\"].str.split(',').str[0].str.len()\n",
    "train_df[\"Title\"] = train_df[\"Name\"].str.split(',').str[1].str.split(' ').str[1]\n",
    "train_df[\"First Name Length\"] = train_df[\"Name\"].str.split(',').str[1].str.split(\". \", regex=False).str[1].str.len()\n",
    "# train_df.drop(axis=1, columns=['Name', 'Cabin', 'Ticket'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52513389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4143cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[\"Title\"] == \"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d6cfc",
   "metadata": {},
   "source": [
    "categorizing the titles into meaningful categories that takes into account marital status which could equate to class as well as how rare the titels are (e.g. \"the\" is for \"the Countess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd414d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\n",
    "    \"Mr.\": \"Mr\",\n",
    "    \"Mrs.\": \"Mrs\",\n",
    "    \"Miss.\": \"Miss\",\n",
    "    \"Ms.\": \"Miss\",       # Unmarried woman (modern)\n",
    "    \"Mlle.\": \"Miss\",     # French for Miss\n",
    "    \"Mme.\": \"Mrs\",       # French for Mrs\n",
    "    \"Master.\": \"Master\", # Usually boys under 12\n",
    "    \"Dr.\": \"Rare\",       # Ambiguous — could be male or female\n",
    "    \"Rev.\": \"Rare\",\n",
    "    \"Major.\": \"Rare\",\n",
    "    \"Col.\": \"Rare\",\n",
    "    \"Capt.\": \"Rare\",\n",
    "    \"Sir.\": \"Rare\",\n",
    "    \"Lady.\": \"Rare\",\n",
    "    \"Don.\": \"Rare\",\n",
    "    \"Jonkheer.\": \"Rare\",\n",
    "    \"the\": \"Rare\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30629b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Title\"] = train_df[\"Title\"].map(title_mapping)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00884f1",
   "metadata": {},
   "source": [
    "fill deck nan with \"U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Deck'].fillna('U', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b3785",
   "metadata": {},
   "source": [
    "make sex into binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Sex'] = train_df['Sex'].map({'male':0, 'female':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec6fa8",
   "metadata": {},
   "source": [
    "ok now help me apply get_dummy prior to correlation heatmap to see if embarked, deck, or title have meaning\n",
    "\n",
    "and then drop name and ticket and cabin columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb07af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at deck survival correlation\n",
    "\n",
    "sns.countplot(x='Deck', hue='Survived', data=train_df)\n",
    "plt.ylim(top=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f262c56",
   "metadata": {},
   "source": [
    "then add dummy data for categories like embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=[\"Deck\", \"Embarked\", \"Title\"])\n",
    "print(train_df.columns)\n",
    "train_df.drop(axis=1, columns=['Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d379750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to answer 'Which features are correlated with survival?' correlation plot\n",
    "\n",
    "corr = train_df.corr()\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84202432",
   "metadata": {},
   "source": [
    "DROPPING\n",
    "\n",
    "**Embarked_Q** → near 0 correlation\n",
    "\n",
    "**Deck_A, Deck_G, Deck_T** → too rare or no predictive value so dropping\n",
    "\n",
    "**Deck_U** → yes it’s correlated, but might reflect missingness bias (people without assigned decks were more likely to die); consider turning it into a binary column like Have_Deck\n",
    "\n",
    "**Title_*** → only Mr, Miss, Mrs were correlated which is redundant with Sex\n",
    "\n",
    "ADDING IN/TWEAKING\n",
    "\n",
    "**Decks B-E** were high survival decks --> add a bucket feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['High Survival Deck'] = (train_df['Deck_B']) | (train_df['Deck_C']) | (train_df['Deck_D']) | (train_df['Deck_E'])\n",
    "train_df.drop(axis=1, columns=['Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T', 'Deck_U' 'Embarked_Q', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare'], inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try again\n",
    "\n",
    "corr = train_df.corr()\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf89666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deck plot\n",
    "\n",
    "sns.barplot(x='Deck', y='Survived', data=train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9add882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218b0b0",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "- Visualize survival by class, sex, age, family, fare\n",
    "- Show correlations (heatmap, groupby stats)\n",
    "- Write observations inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c86ebf0",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "- describe data\n",
    "- Of the estimated 2,224 passengers and crew aboard, approximately 1,500 died (estimates vary) [https://en.wikipedia.org/wiki/Titanic]\n",
    "- 891 entries in the training set\n",
    "- 418 in the test set\n",
    "- 1309 total meaning a rougly 7:3 split for train:test\n",
    "\n",
    "x\n",
    "\n",
    "- Inspect nulls\n",
    "- Drop/recode columns\n",
    "- Feature engineering (like 'is_alone', deck extraction, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c70564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean as needed\n",
    "# maybe remove outliers?\n",
    "\n",
    "\n",
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill age na values\n",
    "train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n",
    "\n",
    "# drop cabin column and name\n",
    "train_df.drop(axis=1, columns='Cabin', inplace=True)\n",
    "\n",
    "# drop embarked na rows\n",
    "train_df.dropna(subset=['Embarked'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c69b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c28a7",
   "metadata": {},
   "source": [
    "optional stuff to refine it later!\n",
    "\n",
    "# 11\n",
    "    pressure_mask = df[\"ap_lo\"] <= df[\"ap_hi\"]\n",
    "    \n",
    "    short_mask = df[\"height\"] >= df[\"height\"].quantile(0.025) \n",
    "    \n",
    "    tall_mask = df[\"height\"] <= df[\"height\"].quantile(0.975)\n",
    "    \n",
    "    low_weight_mask = df[\"weight\"] >= df[\"weight\"].quantile(0.025)\n",
    "    \n",
    "    high_weight_mask = df[\"weight\"] <= df[\"weight\"].quantile(0.975)\n",
    "\n",
    "    df_heat = df[pressure_mask & short_mask & tall_mask & low_weight_mask & high_weight_mask]\n",
    "\n",
    "\n",
    "\n",
    "# Clean data\n",
    "df = df[(df['views'] >= df['views'].quantile(0.025)) & (df['views'] <= df['views'].quantile(0.975))] # 1304 -> 1176\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2efb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get column types????\n",
    "train_df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff26514",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c2f1f",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "- 5 models: Logistic Regression, Decision Tree, Random Forest, KNN, Naive Bayes\n",
    "- Optionally: SVM, Gradient Boosting, or Perceptron\n",
    "- Compare accuracy, precision, recall, AUC\n",
    "- Pick a best model + explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b8388",
   "metadata": {},
   "source": [
    "### 1) Logistic Regression model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log reg code\n",
    "X_train = train_df.copy()\n",
    "X_train.drop(columns=['Name', 'Ticket', 'Survived'], inplace=True)\n",
    "y_train = train_df['Survived']\n",
    "X_test = test_df.copy()\n",
    "X_test.drop(axis=1, columns='Cabin', inplace=True)\n",
    "X_test['Age'].fillna(X_test['Age'].median(), inplace=True)\n",
    "X_test['Fare'].fillna(X_test['Fare'].median(), inplace=True)\n",
    "X_test.drop(columns=['Name', 'Ticket'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all numerical\n",
    "\n",
    "X_train['Sex'] = X_train['Sex'].astype(str).map({\"male\": 0, \"female\": 1})\n",
    "X_test['Sex'] = X_test['Sex'].astype(str).map({\"male\": 0, \"female\": 1})\n",
    "X_train['Embarked'] = X_train['Embarked'].astype(str).map({'Q': 0, 'S': 1, 'C': 2})\n",
    "X_test['Embarked'] = X_test['Embarked'].astype(str).map({'Q': 0, 'S': 1, 'C': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# log reg!\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220269d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('titanic/gender_submission.csv')\n",
    "submission['Survived'] = y_pred\n",
    "submission # 75% --> answer to 'Can we accurately predict survival with a simple model?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('logreg_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982a54e",
   "metadata": {},
   "source": [
    "### 2) Decision Tree model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbba4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f9e50",
   "metadata": {},
   "source": [
    "### 3) Random Forest\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd81cc5",
   "metadata": {},
   "source": [
    "### 4) KNN model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b375ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b42ca",
   "metadata": {},
   "source": [
    "### 5) Naive Bayes model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64785f",
   "metadata": {},
   "source": [
    "### 6) SVM model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1d1c8",
   "metadata": {},
   "source": [
    "### 7) Gradient Boosting model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f315ad",
   "metadata": {},
   "source": [
    "### 8) Perceptron model\n",
    "\n",
    "- description high level w figs (read articles / watch vids)\n",
    "- pros and cons in general and for this dataset\n",
    "\n",
    "[writeup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e02a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698dc7e",
   "metadata": {},
   "source": [
    "### Final Evaluation\n",
    "\n",
    "- Confusion matrix, F1, ROC curve\n",
    "- Feature importance chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cc4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933298b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to answer 'Which model performs best on this dataset?'ArithmeticError\n",
    "\n",
    "best_model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a0f75",
   "metadata": {},
   "source": [
    "[write up too]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69074a5",
   "metadata": {},
   "source": [
    "### Wrap-up\n",
    "- Final thoughts, takeaways\n",
    "- What you’d do next with more time/data\n",
    "- References or inspiration sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253721e6",
   "metadata": {},
   "source": [
    "[writeup]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44ce6e",
   "metadata": {},
   "source": [
    "# References\n",
    "- Titanic - Machine Learning from Disaster [https://www.kaggle.com/competitions/titanic/data]\n",
    "- Titanic (wikipedia) [https://en.wikipedia.org/wiki/Titanic]\n",
    "- cleaning data [https://www.youtube.com/watch?v=cWf08xuSqdU&ab_channel=DataGeekismyname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be9e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7e4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae69482",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
